{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of LlammaIndex for RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to use local modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..')) # Add parent directory to path\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini with LlammaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "from src import utils\n",
    "\n",
    "import pyprojroot\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# TODO continue here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Magic Backpack**\n",
      "\n",
      "In a realm of wonder, where dreams take flight,\n",
      "There lies a backpack, with powers of light.\n",
      "Its fabric gleams, a vibrant hue,\n",
      "A portal to realms both old and new.\n",
      "\n",
      "With every step, it whispers secrets untold,\n",
      "Of adventures waiting, stories to be unrolled.\n",
      "Its compartments hold treasures beyond compare,\n",
      "A world of magic, beyond compare.\n",
      "\n",
      "A book of spells, with pages that ignite,\n",
      "Unveiling secrets, casting spells of light.\n",
      "A compass that guides, through paths unknown,\n",
      "Leading to wonders, where dreams have grown.\n",
      "\n",
      "A telescope that pierces the veil of night,\n",
      "Revealing stars that dance in celestial flight.\n",
      "A wand that grants wishes, with a gentle sway,\n",
      "Transforming dreams into a vibrant display.\n",
      "\n",
      "But beware, young traveler, for power can deceive,\n",
      "The backpack's magic, you must wisely conceive.\n",
      "For with great power, comes great responsibility,\n",
      "Use it for good, with utmost humility.\n",
      "\n",
      "So don the backpack, and embark on your quest,\n",
      "Where wonders await, and dreams are blessed.\n",
      "May its magic guide you, through every stride,\n",
      "As you explore the realms, where dreams reside.\n"
     ]
    }
   ],
   "source": [
    "# Example with complete\n",
    "resp = Gemini().complete(\"Write a poem about a magic backpack\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Ahoy there, matey! Let's set sail on a culinary adventure and find the perfect dinner for ye. What be yer cravings this fine evening? Arr, consider these options:\n",
      "\n",
      "* **Grilled Salmon with Roasted Veggies:** Avast! A hearty and healthy choice that'll keep ye satisfied. Grill up some succulent salmon and pair it with roasted veggies like carrots, potatoes, and asparagus.\n",
      "\n",
      "* **Shepherd's Pie:** Shiver me timbers! A classic comfort food that'll warm ye to the bones. Layer ground beef, veggies, and a creamy mashed potato topping in a baking dish.\n",
      "\n",
      "* **Chicken Tikka Masala:** Ahoy, landlubber! Embark on a flavorful journey with this Indian dish. Tender chicken marinated in yogurt and spices, simmered in a creamy tomato sauce.\n",
      "\n",
      "* **Spaghetti and Meatballs:** Weigh anchor and set course for Italy! Savor the classic combination of tender spaghetti, savory meatballs, and a rich tomato sauce.\n",
      "\n",
      "* **Tacos:** Avast! Ahoy! Ahoy! Set sail for Mexico with these customizable delights. Fill tortillas with your favorite fillings, such as grilled chicken, fish, or veggies, and top with salsa, guacamole, and sour cream.\n",
      "\n",
      "Now, batten down the hatches and let us know what tickles yer fancy. May yer dinner be as grand as the seven seas!\n"
     ]
    }
   ],
   "source": [
    "# Example with chat\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
    "    ChatMessage(role=\"user\", content=\"Help me decide what to have for dinner.\"),\n",
    "]\n",
    "resp = Gemini().chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with streaming completion\n",
    "llm = Gemini()\n",
    "resp = llm.stream_complete(\n",
    "    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the quaint bakery of \"Knead to Know,\" amidst the aroma of freshly baked bread, a peculiar tale unfolded. It began with a humble loaf of sourdough, left to rise overnight in a warm corner.\n",
      "\n",
      "As the yeast danced and multiplied, a strange transformation occurred. The dough began to quiver, its surface bubbling and contorting. To the astonishment of the baker, a tiny creature emerged from the depths of the loaf.\n",
      "\n",
      "With a crusty exterior and a soft, doughy interior, the creature came to be known as Sourcrust. Its eyes sparkled with a mischievous glint, and its tiny hands kneaded the air as if it were shaping an invisible loaf.\n",
      "\n",
      "From that day forward, Sourcrust became an enigmatic presence in the bakery. It would leap from shelf to shelf, leaving a trail of breadcrumbs in its wake. It had a voracious appetite for flour and water, and it possessed an uncanny ability to communicate with the other baked goods.\n",
      "\n",
      "The croissants whispered secrets to Sourcrust, the baguettes shared their tales of adventure, and the muffins confided in it their dreams. Sourcrust became a confidant, a friend, and a source of endless amusement for the inhabitants of the bakery."
     ]
    }
   ],
   "source": [
    "for r in resp:\n",
    "    print(r.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with streaming chat\n",
    "llm = Gemini(model=\"models/gemini-pro\")\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
    "    ChatMessage(role=\"user\", content=\"Help me decide what to have for dinner.\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, matey! Let's set sail on a culinary adventure and find the perfect dinner for ye. What be yer cravings, landlubber?"
     ]
    }
   ],
   "source": [
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP: RAG with llm=Gemini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_inde\n",
    "import faiss\n",
    "\n",
    "# From https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0\n",
    "\n",
    "\n",
    "def process_pdf(pdf):\n",
    "    file = PdfReader(pdf)\n",
    "    text = \"\"\n",
    "    for page in file.pages:\n",
    "        text += str(page.extract_text())\n",
    "\n",
    "    doc = Document(text=text)\n",
    "    return [doc]\n",
    "\n",
    "\n",
    "def get_vector_index(documents):\n",
    "    llm = Gemini(model=\"models/gemini-pro\")\n",
    "    faiss_index = faiss.IndexFlatL2(d)\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, service_context=service_context, storage_context=storage_context\n",
    "    )\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# pdf_file = open(pdf_path, \"rb\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m documents \u001b[38;5;241m=\u001b[39m process_pdf(pdf_path)\n\u001b[0;32m----> 4\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mget_vector_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m engine \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_query_engine()\n\u001b[1;32m      6\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow has Microsoft performed in this fiscal year?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[53], line 22\u001b[0m, in \u001b[0;36mget_vector_index\u001b[0;34m(documents)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector_index\u001b[39m(documents):\n\u001b[1;32m     21\u001b[0m     llm \u001b[38;5;241m=\u001b[39m Gemini(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/gemini-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     faiss_index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(\u001b[43md\u001b[49m)\n\u001b[1;32m     23\u001b[0m     vector_store \u001b[38;5;241m=\u001b[39m FaissVectorStore(faiss_index\u001b[38;5;241m=\u001b[39mfaiss_index)\n\u001b[1;32m     25\u001b[0m     storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store\u001b[38;5;241m=\u001b[39mvector_store)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "pdf_path = pyprojroot.here(\"data/vaswani_et_al_2017.pdf\")\n",
    "# pdf_file = open(pdf_path, \"rb\")\n",
    "documents = process_pdf(pdf_path)\n",
    "index = get_vector_index(documents)\n",
    "engine = index.as_query_engine()\n",
    "query = \"How has Microsoft performed in this fiscal year?\"\n",
    "response = engine(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative attempt\n",
    "https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12615/2461464375.py:20: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/documents/GitHub/arxiv-chat/venv/lib/python3.11/site-packages/llama_index/core/embeddings/utils.py:59\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     58\u001b[0m     embed_model \u001b[38;5;241m=\u001b[39m OpenAIEmbedding()\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/documents/GitHub/arxiv-chat/venv/lib/python3.11/site-packages/llama_index/embeddings/openai/utils.py:104\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m     index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(docs, service_context\u001b[38;5;241m=\u001b[39mservice_context)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m index\n\u001b[0;32m---> 25\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 20\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m reader \u001b[38;5;241m=\u001b[39m SimpleDirectoryReader(input_dir\u001b[38;5;241m=\u001b[39mpdf_path, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m docs \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m---> 20\u001b[0m service_context \u001b[38;5;241m=\u001b[39m \u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(docs, service_context\u001b[38;5;241m=\u001b[39mservice_context)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index\n",
      "File \u001b[0;32m~/documents/GitHub/arxiv-chat/venv/lib/python3.11/site-packages/deprecated/classic.py:285\u001b[0m, in \u001b[0;36mdeprecated.<locals>.wrapper_function\u001b[0;34m(wrapped_, instance_, args_, kwargs_)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39mcategory, stacklevel\u001b[38;5;241m=\u001b[39m_routine_stacklevel)\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/documents/GitHub/arxiv-chat/venv/lib/python3.11/site-packages/llama_index/core/service_context.py:200\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m         llm_predictor\u001b[38;5;241m.\u001b[39mquery_wrapper_prompt \u001b[38;5;241m=\u001b[39m query_wrapper_prompt\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# NOTE: the embed_model isn't used in all indices\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# NOTE: embed model should be a transformation, but the way the service\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# context works, we can't put in there yet.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m embed_model\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n\u001b[1;32m    203\u001b[0m prompt_helper \u001b[38;5;241m=\u001b[39m prompt_helper \u001b[38;5;129;01mor\u001b[39;00m _get_default_prompt_helper(\n\u001b[1;32m    204\u001b[0m     llm_metadata\u001b[38;5;241m=\u001b[39mllm_predictor\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    205\u001b[0m     context_window\u001b[38;5;241m=\u001b[39mcontext_window,\n\u001b[1;32m    206\u001b[0m     num_output\u001b[38;5;241m=\u001b[39mnum_output,\n\u001b[1;32m    207\u001b[0m )\n",
      "File \u001b[0;32m~/documents/GitHub/arxiv-chat/venv/lib/python3.11/site-packages/llama_index/core/embeddings/utils.py:66\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-embeddings-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-embeddings-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 66\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI embedding model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConsider using embed_model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisit our documentation for more embedding options: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.llamaindex.ai/en/stable/module_guides/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings.html#modules\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# for image multi-modal embeddings\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m embed_model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    Document,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "\n",
    "\n",
    "# @st.cache_resource(show_spinner=False)\n",
    "def load_data():\n",
    "    llm = Gemini(model=\"models/gemini-pro\")\n",
    "    # pdf_path = pyprojroot.here(\"data/vaswani_et_al_2017.pdf\")\n",
    "    pdf_path = pyprojroot.here(\"data\")\n",
    "    reader = SimpleDirectoryReader(input_dir=pdf_path, recursive=True)\n",
    "    docs = reader.load_data()\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    index = VectorStoreIndex.from_documents(docs, service_context=service_context)\n",
    "    return index\n",
    "\n",
    "\n",
    "index = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
