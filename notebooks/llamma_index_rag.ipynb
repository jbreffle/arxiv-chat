{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of LlammaIndex for RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Set up to use local modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini with LlammaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "from src import utils\n",
    "\n",
    "import pyprojroot\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# TODO continue here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82087/3917234533.py:2: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/This package will no longer be supported after version 0.6.2) -- Deprecated since version 0.6.2.\n",
      "  resp = Gemini().complete(\"Write a poem about a magic backpack\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Wanderer's Pack\n",
      "\n",
      "Old leather, worn and patched with thread,\n",
      "A simple backpack, softly bred.\n",
      "It doesn't boast of shining gleam,\n",
      "But holds within a waking dream.\n",
      "\n",
      "For this is no ordinary hold,\n",
      "A whispered secret, centuries old.\n",
      "It shifts and breathes, a gentle hum,\n",
      "Whatever you need, will surely come.\n",
      "\n",
      "Lost in the woods? A map appears,\n",
      "With glowing trails to quell your fears.\n",
      "Hungry and cold? A warming stew,\n",
      "And blankets soft, and skies of blue.\n",
      "\n",
      "A broken heart? A comforting rhyme,\n",
      "To mend the pieces, one at a time.\n",
      "A daunting task? A tool so bright,\n",
      "To guide your hand and set things right.\n",
      "\n",
      "But heed this warning, soft and low,\n",
      "The pack gives freely, as you grow.\n",
      "It doesn't grant a wishful plea,\n",
      "But what you *need* to truly be.\n",
      "\n",
      "It asks for kindness, courage true,\n",
      "And a willingness to see things through.\n",
      "So wear it lightly, with open eyes,\n",
      "The Wanderer's Pack, a sweet surprise. \n",
      "\n",
      "For magic isn't found in things,\n",
      "But in the journey that living brings.\n",
      "And this old backpack, wise and deep,\n",
      "Will help you wander, while you sleep...\n",
      "And while you wake, and bravely leap.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example with complete\n",
    "resp = Gemini().complete(\"Write a poem about a magic backpack\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82087/3443047961.py:7: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/This package will no longer be supported after version 0.6.2) -- Deprecated since version 0.6.2.\n",
      "  resp = Gemini().chat(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Okay, awesome! To give you the *best* recommendation, tell me a little about what you're feeling. But here are a bunch of ideas to get us started, broken down by effort/mood:\n",
      "\n",
      "**1. Super Easy (15 minutes or less):**\n",
      "\n",
      "* **Pasta with Pesto:** Seriously quick and satisfying.\n",
      "* **Quesadillas:** Endless filling possibilities!\n",
      "* **Salad with Grilled Chicken/Fish/Tofu:** Healthy and fast.\n",
      "* **Soup & Sandwich:** Classic comfort food.\n",
      "* **Eggs & Toast:** Never underestimate the power of breakfast for dinner!\n",
      "\n",
      "**2. Medium Effort (20-45 minutes):**\n",
      "\n",
      "* **Sheet Pan Dinner:** Roast veggies and protein (chicken, sausage, salmon) all on one pan!\n",
      "* **Tacos:** Always a crowd-pleaser.\n",
      "* **Stir-Fry:** Use pre-cut veggies to save time.\n",
      "* **Chicken/Veggie Curry:**  Can be made relatively quickly with curry paste.\n",
      "* **Burgers:** Homemade or store-bought patties.\n",
      "\n",
      "**3. More Involved (45+ minutes - for when you're feeling ambitious!):**\n",
      "\n",
      "* **Lasagna:** A classic, but takes time.\n",
      "* **Roast Chicken:**  Delicious and impressive.\n",
      "* **Shepherd's Pie:** Comfort food at its finest.\n",
      "* **Homemade Pizza:** Fun to make!\n",
      "\n",
      "**Now, to narrow it down, tell me:**\n",
      "\n",
      "* **What kind of cuisine are you in the mood for?** (Italian, Mexican, Asian, American, etc.)\n",
      "* **How much time do you want to spend cooking?** (Quick & easy, or something more involved?)\n",
      "* **What ingredients do you already have on hand?** (This is helpful so I don't suggest something you can't make!)\n",
      "* **Are there any foods you *don't* like or are allergic to?**\n",
      "\n",
      "\n",
      "\n",
      "Let's find you a delicious dinner! âœ¨\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example with chat\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
    "    ChatMessage(role=\"user\", content=\"Help me decide what to have for dinner.\"),\n",
    "]\n",
    "resp = Gemini().chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82087/789428070.py:2: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/This package will no longer be supported after version 0.6.2) -- Deprecated since version 0.6.2.\n",
      "  llm = Gemini()\n"
     ]
    }
   ],
   "source": [
    "# Example with streaming completion\n",
    "llm = Gemini()\n",
    "resp = llm.stream_complete(\n",
    "    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for r in resp:\n",
    "        print(r.text, end=\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing response chunk: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82087/3987730226.py:2: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/This package will no longer be supported after version 0.6.2) -- Deprecated since version 0.6.2.\n",
      "  llm = Gemini(model=\"models/gemma-3-27b-it\")\n"
     ]
    }
   ],
   "source": [
    "# Example with streaming chat\n",
    "llm = Gemini(model=\"models/gemma-3-27b-it\")\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
    "    ChatMessage(role=\"user\", content=\"Help me decide what to have for dinner.\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for r in resp:\n",
    "        print(r.delta, end=\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing response chunk: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP: RAG with llm=Gemini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "# From https://www.llamaindex.ai/blog/how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0\n",
    "\n",
    "\n",
    "def process_pdf(pdf):\n",
    "    file = PdfReader(pdf)\n",
    "    text = \"\"\n",
    "    for page in file.pages:\n",
    "        text += str(page.extract_text())\n",
    "\n",
    "    doc = Document(text=text)\n",
    "    return [doc]\n",
    "\n",
    "\n",
    "def get_vector_index(documents):\n",
    "    llm = Gemini(model=\"models/gemma-3-27b-it\")\n",
    "    faiss_index = faiss.IndexFlatL2(d)\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, service_context=service_context, storage_context=storage_context\n",
    "    )\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82087/4082111283.py:21: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/This package will no longer be supported after version 0.6.2) -- Deprecated since version 0.6.2.\n",
      "  llm = Gemini(model=\"models/gemma-3-27b-it\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# pdf_file = open(pdf_path, \"rb\")\u001b[39;00m\n\u001b[32m      3\u001b[39m documents = process_pdf(pdf_path)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m index = \u001b[43mget_vector_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m engine = index.as_query_engine()\n\u001b[32m      6\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mHow has Microsoft performed in this fiscal year?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mget_vector_index\u001b[39m\u001b[34m(documents)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_vector_index\u001b[39m(documents):\n\u001b[32m     21\u001b[39m     llm = Gemini(model=\u001b[33m\"\u001b[39m\u001b[33mmodels/gemma-3-27b-it\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     faiss_index = faiss.IndexFlatL2(\u001b[43md\u001b[49m)\n\u001b[32m     23\u001b[39m     vector_store = FaissVectorStore(faiss_index=faiss_index)\n\u001b[32m     25\u001b[39m     storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
      "\u001b[31mNameError\u001b[39m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "pdf_path = pyprojroot.here(\"data/vaswani_et_al_2017.pdf\")\n",
    "# pdf_file = open(pdf_path, \"rb\")\n",
    "documents = process_pdf(pdf_path)\n",
    "index = get_vector_index(documents)\n",
    "engine = index.as_query_engine()\n",
    "query = \"How has Microsoft performed in this fiscal year?\"\n",
    "response = engine(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative attempt\n",
    "https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82087/3094350285.py:14: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/This package will no longer be supported after version 0.6.2) -- Deprecated since version 0.6.2.\n",
      "  llm = Gemini(model=\"models/gemma-3-27b-it\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.\nSee the docs for updated usage/migration: \nhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     20\u001b[39m     index = VectorStoreIndex.from_documents(docs, service_context=service_context)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m index\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m index = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m reader = SimpleDirectoryReader(input_dir=pdf_path, recursive=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m docs = reader.load_data()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m service_context = \u001b[43mServiceContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m index = VectorStoreIndex.from_documents(docs, service_context=service_context)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/documents/GitHub/arxiv-chat/.venv/lib/python3.11/site-packages/llama_index/core/service_context.py:33\u001b[39m, in \u001b[36mServiceContext.from_defaults\u001b[39m\u001b[34m(cls, **kwargs)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_defaults\u001b[39m(\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m     24\u001b[39m     **kwargs: Any,\n\u001b[32m     25\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mServiceContext\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Create a ServiceContext from defaults.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mServiceContext is deprecated. Use llama_index.settings.Settings instead, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor pass in modules to local functions/methods/interfaces.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSee the docs for updated usage/migration: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.\nSee the docs for updated usage/migration: \nhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    Document,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "\n",
    "\n",
    "# @st.cache_resource(show_spinner=False)\n",
    "def load_data():\n",
    "    llm = Gemini(model=\"models/gemma-3-27b-it\")\n",
    "    # pdf_path = pyprojroot.here(\"data/vaswani_et_al_2017.pdf\")\n",
    "    pdf_path = pyprojroot.here(\"data\")\n",
    "    reader = SimpleDirectoryReader(input_dir=pdf_path, recursive=True)\n",
    "    docs = reader.load_data()\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    index = VectorStoreIndex.from_documents(docs, service_context=service_context)\n",
    "    return index\n",
    "\n",
    "\n",
    "index = load_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
