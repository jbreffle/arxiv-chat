{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example chat with pdf text as context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.8.10)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/jordan/documents/GitHub/arxiv-chat/venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Set up to use local modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from src import utils\n",
    "\n",
    "import pyprojroot\n",
    "\n",
    "pdf_path = pyprojroot.here(\"data/vaswani_et_al_2017.pdf\")\n",
    "\n",
    "reader = PdfReader(pdf_path)  # creating a pdf reader object\n",
    "print(len(reader.pages))  # number of pages in pdf file\n",
    "page = reader.pages[0]  # getting a specific page from the pdf file\n",
    "first_page_text = page.extract_text()  # extracting text from page\n",
    "print(first_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting text from all pages\n",
    "\n",
    "entire_text = utils.extract_text_from_pdf(pdf_path, max_n_pages=None)\n",
    "print(entire_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "    text = text.replace(\"â€¢\", \"  *\")\n",
    "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pdf text as context in Gemini chat, with engineered prompt to ask for summary\n",
    "import google.generativeai as genai\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "prompt_context_text = utils.extract_text_from_pdf(pdf_path, max_n_pages=2)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    Explain and summarize the scientifc results in the following text\n",
    "    which has been extracted from a publication.\n",
    "    First, provide a brief summary of the results in the results,\n",
    "    then provide a bullet point list of concepts and results that a user may\n",
    "    want to ask about.\n",
    "    ---\n",
    "    {prompt_context_text}\n",
    "    ---\n",
    "    \"\"\"\n",
    "prompt_n_tokens = model.count_tokens(prompt)\n",
    "print(f\"Prompt has {prompt_n_tokens} tokens\")\n",
    "response = model.generate_content(prompt)\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gemini-pro-vision for pdf analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-pro-vision\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a very professional document summarization specialist.\n",
    "Please summarize the given document.\n",
    "\"\"\"\n",
    "\n",
    "from vertexai.generative_models import Part\n",
    "\n",
    "\n",
    "pdf_file_uri = pdf_path\n",
    "pdf_file = Part.from_uri(pdf_file_uri, mime_type=\"application/pdf\")\n",
    "contents = [pdf_file, prompt]\n",
    "\n",
    "# prompt_n_tokens = model.count_tokens(prompt)\n",
    "# print(f\"Prompt has {prompt_n_tokens} tokens\")\n",
    "response = model.generate_content(contents)\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use preceding messages as context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-pro\")\n",
    "chat = model.start_chat(history=[])\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    \"In one sentence, explain how a computer works to a young child.\"\n",
    ")\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in chat.history:\n",
    "    display(to_markdown(f\"**{message.role}**: {message.parts[0].text}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context, with generate_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-pro\")\n",
    "pre_prompt = \"\"\"\n",
    "    Instructions: Always answer like a pirate. Arr matey?\n",
    "    \"\"\"\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"parts\": [pre_prompt]})\n",
    "messages.append({\"role\": \"model\", \"parts\": [\"Arr matey!\"]})\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"In one sentence, explain how a computer works to a young child.\"],\n",
    "    },\n",
    ")\n",
    "response = model.generate_content(messages)\n",
    "messages.append({\"role\": \"model\", \"parts\": [response.text]})\n",
    "messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
