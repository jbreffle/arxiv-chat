{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pdfs from arxiv.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to use local modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprojroot\n",
    "\n",
    "from src import utils\n",
    "\n",
    "PDF_DIR = pyprojroot.here(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravitational Background of Alice-Vortices and R7-Branes\n",
      "Matter-induced plaquette terms in a $\\mathbb{Z}_2$ lattice gauge theory\n",
      "Nuclear gradients from auxiliary-field quantum Monte Carlo and their application in geometry optimization and transition state search\n",
      "Discrete Invariants of Koszul Artin-Schelter Regular Algebras of Dimension four\n",
      "Absorption imaging of quantum gases near surfaces using incoherent light\n",
      "Non-chiral ephemeral edge states and cascading of exceptional points in the non-reciprocal Haldane model\n",
      "Mean-Force Hamiltonians from Influence Functionals\n",
      "Single snapshot non-Markovianity of Pauli channels\n",
      "An updated constraint for the Gravitational Wave Background from the Gamma-ray Pulsar Timing Array\n",
      "Emergent aperiodicity in Bose-Bose mixtures induced by spin-dependent periodic potentials\n"
     ]
    }
   ],
   "source": [
    "# https://lukasschwab.me/arxiv.py/arxiv.html\n",
    "\n",
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "search = arxiv.Search(\n",
    "    query=\"quantum\", max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "\n",
    "# `results` is a generator; you can iterate over its elements one by one...\n",
    "for r in client.results(search):\n",
    "    print(r.title)\n",
    "# ...or exhaust it into a list. Careful: this is slow for large results sets.\n",
    "# all_results = list(results)\n",
    "# print([r.title for r in all_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jordan/documents/GitHub/arxiv-chat/data/example_paper.pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_ids = [\"1706.03762v6\", \"1605.08386v1\"]\n",
    "\n",
    "# Search for the paper with ID \"1605.08386v1\"\n",
    "search_by_id = arxiv.Search(id_list=[pdf_ids[0]])\n",
    "paper = next(client.results(search_by_id))\n",
    "print(paper.title)\n",
    "\n",
    "paper.download_pdf(dirpath=PDF_DIR, filename=\"example_paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded: Attention is All You Need\n",
      "Already downloaded: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "Already downloaded: Generative Adversarial Nets\n",
      "Already downloaded: Playing Atari with Deep Reinforcement Learning\n",
      "Already downloaded: ImageNet Classification with Deep Convolutional Neural Networks\n"
     ]
    }
   ],
   "source": [
    "papers = utils.local_papers\n",
    "\n",
    "utils.get_local_papers(papers=papers, silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most recent ML papers from arxiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos\n",
      "The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.\n",
      "Selection of CMIP6 Models for Regional Precipitation Projection and Climate Change Assessment in the Jhelum and Chenab River Basins\n",
      "Effective water resource management depends on accurate projections of flows in water channels. For projected climate data, use of different General Circulation Models (GCM) simulates contrasting results. This study shows selection of GCM for the latest generation CMIP6 for hydroclimate change impact studies. Envelope based method was used for the selection, which includes components based on machine learning techniques, allowing the selection of GCMs without the need for in-situ reference data. According to our knowledge, for the first time, such a comparison was performed for the CMIP6 Shared Socioeconomic Pathway (SSP) scenarios data. In addition, the effect of climate change under SSP scenarios was studied, along with the calculation of extreme indices. Finally, GCMs were compared to quantify spatiotemporal differences between CMIP5 and CMIP6 data. Results provide NorESM2 LM, FGOALS g3 as selected models for the Jhelum and Chenab River. Highly vulnerable regions under the effect of climate change were highlighted through spatial maps, which included parts of Punjab, Jammu, and Kashmir. Upon comparison of CMIP5 and CMIP6, no discernible difference was found between the RCP and SSP scenarios precipitation projections. In the future, more detailed statistical comparisons could further reinforce the proposition.\n",
      "Improved Regret Guarantees for Online Mirror Descent using a Portfolio of Mirror Maps\n",
      "OMD and its variants give a flexible framework for OCO where the performance depends crucially on the choice of the mirror map. While the geometries underlying OPGD and OEG, both special cases of OMD, are well understood, it remains a challenging open question on how to construct an optimal mirror map for any given constrained set and a general family of loss functions, e.g., sparse losses. Motivated by parameterizing a near-optimal set of mirror maps, we consider a simpler question: is it even possible to obtain polynomial gains in regret by using mirror maps for geometries that interpolate between $L_1$ and $L_2$, which may not be possible by restricting to only OEG ($L_1$) or OPGD ($L_2$).\n",
      "  Our main result answers this question positively. We show that mirror maps based on block norms adapt better to the sparsity of loss functions, compared to previous $L_p$ (for $p \\in [1, 2]$) interpolations. In particular, we construct a family of online convex optimization instances in $\\mathbb{R}^d$, where block norm-based mirror maps achieve a provable polynomial (in $d$) improvement in regret over OEG and OPGD for sparse loss functions. We then turn to the setting in which the sparsity level of the loss functions is unknown. In this case, the choice of geometry itself becomes an online decision problem. We first show that naively switching between OEG and OPGD can incur linear regret, highlighting the intrinsic difficulty of geometry selection. To overcome this issue, we propose a meta-algorithm based on multiplicative weights that dynamically selects among a family of uniform block norms. We show that this approach effectively tunes OMD to the sparsity of the losses, yielding adaptive regret guarantees. Overall, our results demonstrate that online mirror-map selection can significantly enhance the ability of OMD to exploit sparsity in online convex optimization.\n",
      "Learning functional components of PDEs from data using neural networks\n",
      "Partial differential equations often contain unknown functions that are difficult or impossible to measure directly, hampering our ability to derive predictions from the model. Workflows for recovering scalar PDE parameters from data are well studied: here we show how similar workflows can be used to recover functions from data. Specifically, we embed neural networks into the PDE and show how, as they are trained on data, they can approximate unknown functions with arbitrary accuracy. Using nonlocal aggregation-diffusion equations as a case study, we recover interaction kernels and external potentials from steady state data. Specifically, we investigate how a wide range of factors, such as the number of available solutions, their properties, sampling density, and measurement noise, affect our ability to successfully recover functions. Our approach is advantageous because it can utilise standard parameter-fitting workflows, and in that the trained PDE can be treated as a normal PDE for purposes such as generating system predictions.\n",
      "Realistic Face Reconstruction from Facial Embeddings via Diffusion Models\n",
      "With the advancement of face recognition (FR) systems, privacy-preserving face recognition (PPFR) systems have gained popularity for their accurate recognition, enhanced facial privacy protection, and robustness to various attacks. However, there are limited studies to further verify privacy risks by reconstructing realistic high-resolution face images from embeddings of these systems, especially for PPFR. In this work, we propose the face embedding mapping (FEM), a general framework that explores Kolmogorov-Arnold Network (KAN) for conducting the embedding-to-face attack by leveraging pre-trained Identity-Preserving diffusion model against state-of-the-art (SOTA) FR and PPFR systems. Based on extensive experiments, we verify that reconstructed faces can be used for accessing other real-word FR systems. Besides, the proposed method shows the robustness in reconstructing faces from the partial and protected face embeddings. Moreover, FEM can be utilized as a tool for evaluating safety of FR and PPFR systems in terms of privacy leakage. All images used in this work are from public datasets.\n",
      "Learning to Approximate Uniform Facility Location via Graph Neural Networks\n",
      "There has been a growing interest in using neural networks, especially message-passing neural networks (MPNNs), to solve hard combinatorial optimization problems heuristically. However, existing learning-based approaches for hard combinatorial optimization tasks often rely on supervised training data, reinforcement learning, or gradient estimators, leading to significant computational overhead, unstable training, or a lack of provable performance guarantees. In contrast, classical approximation algorithms offer such performance guarantees under worst-case inputs but are non-differentiable and unable to adaptively exploit structural regularities in natural input distributions. We address this dichotomy with the fundamental example of Uniform Facility Location (UniFL), a variant of the combinatorial facility location problem with applications in clustering, data summarization, logistics, and supply chain design. We develop a fully differentiable MPNN model that embeds approximation-algorithmic principles while avoiding the need for solver supervision or discrete relaxations. Our approach admits provable approximation and size generalization guarantees to much larger instances than seen during training. Empirically, we show that our approach outperforms standard non-learned approximation algorithms in terms of solution quality, closing the gap with computationally intensive integer linear programming approaches. Overall, this work provides a step toward bridging learning-based methods and approximation algorithms for discrete optimization.\n",
      "Quantization-Robust LLM Unlearning via Low-Rank Adaptation\n",
      "Large Language Model (LLM) unlearning aims to remove targeted knowledge from a trained model, but practical deployments often require post-training quantization (PTQ) for efficient inference. However, aggressive low-bit PTQ can mask or erase unlearning updates, causing quantized models to revert to pre-unlearning behavior. We show that standard full-parameter fine-tuning often induce parameter changes that are too small to survive 4-bit quantization. We propose quantization-robust unlearning via low-rank adaptation (LoRA): we freeze the base model and concentrate unlearning into trainable adapters so that the effective update is preserved after quantization. On Llama-2-7B evaluated with MUSE dataset (BOOKS and NEWS), LoRA improves 4-bit utility by up to 7.93 points (NPO+GDR on BOOKS: 50.17 to 58.10) and yields higher 4-bit utility on NEWS for GA+GDR (40.06 to 44.82, increase of 4.76). LoRA also substantially reduces privacy leakage under 4-bit PTQ, e.g., for GA+KLR on BOOKS, PrivLeak moves from -25.68 to -5.86 (closer to ideal 0), while maintaining strong forgetting (VerMem and KnowMem near 0). Thus, using LoRA for Machine Unlearning is beneficial for scenarios where quantization is necessary for model deployment.\n",
      "FlashSchNet: Fast and Accurate Coarse-Grained Neural Network Molecular Dynamics\n",
      "Graph neural network (GNN) potentials such as SchNet improve the accuracy and transferability of molecular dynamics (MD) simulation by learning many-body interactions, but remain slower than classical force fields due to fragmented kernels and memory-bound pipelines that underutilize GPUs. We show that a missing principle is making GNN-MD IO-aware, carefully accounting for reads and writes between GPU high-bandwidth memory (HBM) and on-chip SRAM. We present FlashSchNet, an efficient and accurate IO-aware SchNet-style GNN-MD framework built on four techniques: (1) flash radial basis, which fuses pairwise distance computation, Gaussian basis expansion, and cosine envelope into a single tiled pass, computing each distance once and reusing it across all basis functions; (2) flash message passing, which fuses cutoff, neighbor gather, filter multiplication, and reduction to avoid materializing edge tensors in HBM; (3) flash aggregation, which reformulates scatter-add via CSR segment reduce, reducing atomic writes by a factor of feature dimension and enabling contention-free accumulation in both forward and backward passes; (4) channel-wise 16-bit quantization that exploits the low per-channel dynamic range in SchNet MLP weights to further improve throughput with negligible accuracy loss. On a single NVIDIA RTX PRO 6000, FlashSchNet achieves 1000 ns/day aggregate simulation throughput over 64 parallel replicas on coarse-grained (CG) protein containing 269 beads (6.5x faster than CGSchNet baseline with 80% reduction of peak memory), surpassing classical force fields (e.g. MARTINI) while retaining SchNet-level accuracy and transferability.\n",
      "Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching\n",
      "Template-free retrosynthesis methods treat the task as black-box sequence generation, limiting learning efficiency, while semi-template approaches rely on rigid reaction libraries that constrain generalization. We address this gap with a key insight: atom ordering in neural representations matters. Building on this insight, we propose a structure-aware template-free framework that encodes the two-stage nature of chemical reactions as a positional inductive bias. By placing reaction center atoms at the sequence head, our method transforms implicit chemical knowledge into explicit positional patterns that the model can readily capture. The proposed RetroDiT backbone, a graph transformer with rotary position embeddings, exploits this ordering to prioritize chemically critical regions. Combined with discrete flow matching, our approach decouples training from sampling and enables generation in 20--50 steps versus 500 for prior diffusion methods. Our method achieves state-of-the-art performance on both USPTO-50k (61.2% top-1) and the large-scale USPTO-Full (51.3% top-1) with predicted reaction centers. With oracle centers, performance reaches 71.1% and 63.4% respectively, surpassing foundation models trained on 10 billion reactions while using orders of magnitude less data. Ablation studies further reveal that structural priors outperform brute-force scaling: a 280K-parameter model with proper ordering matches a 65M-parameter model without it.\n",
      "Eventizing Traditionally Opaque Binary Neural Networks as 1-safe Petri net Models\n",
      "Binary Neural Networks (BNNs) offer a low-complexity and energy-efficient alternative to traditional full-precision neural networks by constraining their weights and activations to binary values. However, their discrete, highly non-linear behavior makes them difficult to explain, validate and formally verify. As a result, BNNs remain largely opaque, limiting their suitability in safety-critical domains, where causal transparency and behavioral guarantees are essential. In this work, we introduce a Petri net (PN)-based framework that captures the BNN's internal operations as event-driven processes. By \"eventizing\" their operations, we expose their causal relationships and dependencies for a fine-grained analysis of concurrency, ordering, and state evolution. Here, we construct modular PN blueprints for core BNN components including activation, gradient computation and weight updates, and compose them into a complete system-level model. We then validate the composed PN against a reference software-based BNN, verify it against reachability and structural checks to establish 1-safeness, deadlock-freeness, mutual exclusion and correct-by-construction causal sequencing, before we assess its scalability and complexity at segment, component, and system levels using the automated measurement tools in Workcraft. Overall, this framework enables causal introspection of transparent and event-driven BNNs that are amenable to formal reasoning and verification.\n"
     ]
    }
   ],
   "source": [
    "# Use the `arxiv` package to get the 10 most recent papers on the topic of cs.LG\n",
    "# Papers categorized with stat.ML as primary are automatically cross-listed as cs.LG but not vice versa.\n",
    "# Computer science: Machine Learning = cs.LG\n",
    "# Other,\n",
    "# Statistics: Machine Learning = stat.ML\n",
    "# Computer science: Artificial Intelligence = cs.AI\n",
    "# Computer science: Neural and Evolutionary Computing = cs.NE\n",
    "# Computer science: Systems and Control = cs.SY\n",
    "# Math: Optimization and Control = math.OC\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"cs.LG\", max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "# Print the titles\n",
    "for r in results:\n",
    "    print(r.title)\n",
    "    print(r.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first paper from results\n",
    "paper = next(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arxiv.Result(entry_id='http://arxiv.org/abs/2602.13197v1', updated=datetime.datetime(2026, 2, 13, 18, 59, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2026, 2, 13, 18, 59, 10, tzinfo=datetime.timezone.utc), title='Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos', authors=[arxiv.Result.Author('Albert J. Zhai'), arxiv.Result.Author('Kuo-Hao Zeng'), arxiv.Result.Author('Jiasen Lu'), arxiv.Result.Author('Ali Farhadi'), arxiv.Result.Author('Shenlong Wang'), arxiv.Result.Author('Wei-Chiu Ma')], summary=\"The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.\", comment=None, journal_ref=None, doi=None, primary_category='cs.RO', categories=['cs.RO', 'cs.CV', 'cs.LG'], links=[arxiv.Result.Link('https://arxiv.org/abs/2602.13197v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('https://arxiv.org/pdf/2602.13197v1', title='pdf', rel='related', content_type=None)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos\n",
      "2026-02-13 18:59:10+00:00\n",
      "Selection of CMIP6 Models for Regional Precipitation Projection and Climate Change Assessment in the Jhelum and Chenab River Basins\n",
      "2026-02-13 18:41:40+00:00\n",
      "Improved Regret Guarantees for Online Mirror Descent using a Portfolio of Mirror Maps\n",
      "2026-02-13 18:37:26+00:00\n",
      "Learning functional components of PDEs from data using neural networks\n",
      "2026-02-13 18:32:33+00:00\n",
      "Realistic Face Reconstruction from Facial Embeddings via Diffusion Models\n",
      "2026-02-13 18:28:24+00:00\n",
      "Learning to Approximate Uniform Facility Location via Graph Neural Networks\n",
      "2026-02-13 18:08:23+00:00\n",
      "Quantization-Robust LLM Unlearning via Low-Rank Adaptation\n",
      "2026-02-13 18:01:40+00:00\n",
      "FlashSchNet: Fast and Accurate Coarse-Grained Neural Network Molecular Dynamics\n",
      "2026-02-13 17:49:12+00:00\n",
      "Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching\n",
      "2026-02-13 17:39:21+00:00\n",
      "Eventizing Traditionally Opaque Binary Neural Networks as 1-safe Petri net Models\n",
      "2026-02-13 17:25:47+00:00\n"
     ]
    }
   ],
   "source": [
    "# Get all ML papers from today\n",
    "\n",
    "# https://export.arxiv.org/api/query?search_query=cat:cs.LG+AND+submittedDate:[202001130630+TO+202001131645]\n",
    "\n",
    "query = \"cs.LG\"\n",
    "# query by submitteDate is not implemented in the Python API\n",
    "# query = \"cat:cs.LG+AND+submittedDate:[202001130630+TO+202101131645]\"\n",
    "search = arxiv.Search(\n",
    "    query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "# Print the titles\n",
    "for r in results:\n",
    "    print(r.title)\n",
    "    print(r.published)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
